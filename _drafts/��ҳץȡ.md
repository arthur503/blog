---
layout: default
title: 网页内容抓取：1010
---
主要目标：实现网站网页小说分类自动下载更新。




分解目标：

一、实现下载

包括：http连接、HTML分析寻找到内容网址、正则表达式提取网页内容、保存到文件、文件append添加。

二、多线程下载

使用taskmanager进行任务管理；

三、自动更新

利用布隆过滤器进行查重；


任务进度：

* 1010日：HTML网页下载、人工找到网址、保存到到文件；
* 1011日：从目录页面解析章节网址并自动下载保存、Unicode与字符串转换、整理了Site、Category、Book、Chapter的目录结构；还差：每个网页弄完后即是下载？？？多线程？？？


1019:

有一个页面（>>>>ERROR in parse Chapter 第二十章罗升追踪（4）:http://read.qidian.com/BookReader/2998840,48207592.aspx Add to tasklist again. Task Id:6646 retryCounter:9 giveUpCounter:0）实际中为空，在抓取时一直出现错误。因此，导致一直循环抓取该页面。之后，使用bloomfilter存储重复抓取的页面，若为新url，则再次抓取；若已经抓取过一次但还是出错，则放弃。



代码记录：

1010日代码如下：

---

package com.arthur.crawler;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.OutputStreamWriter;
import java.io.Writer;

public class Main {
	
	public static void main(String[] argv){
		
		String url = "http://www.baidu.com";
//		url = "http://arthur503.github.io/blog/2013/10/10/Java-OutOfMemoryError-and-StackOverflowError.html";
//		url = "http://bbs.csdn.net/topics/100047708";
		url = "http://read.qidian.com/BookReader/2524770,44092315.aspx";
//		url = "http://files.qidian.com/Author3/2524770/44092315.txt";
		url = "http://read.qidian.com/BookReader/2524770,44212419.aspx";
		
		String txtUrl = "http://files.qidian.com/Author3/2524770/44092315.txt";
		txtUrl = "http://files.qidian.com/Author3/2524770/44212419.txt";
		
		Crawler crawler = new Crawler();
		String htmlCode = crawler.getHtmlCode(url);
		String txtCode = crawler.getTextCode(txtUrl);
		System.out.println("HTML CODE:\n"+htmlCode);
		System.out.println("TEXT CODE:\n"+txtCode);
		
		File htmlFile = new File("D:\\data\\webcraw\\test1.txt");
		File txtFile = new File("D:\\data\\webcraw\\test2.txt");
		if(!htmlFile.exists()){
			System.out.println("File Not Exist!");
		}
		try {
//			Writer bw = new OutputStreamWriter(new FileOutputStream("test.txt"),"GB2312");
			Writer bw = new OutputStreamWriter(new FileOutputStream(htmlFile));
			bw.write(htmlCode);
			
			bw = new OutputStreamWriter(new FileOutputStream(txtFile));
			bw.write(txtCode);			
			
			bw.close();
			System.out.println(">>Write File Done!");
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}

}

---

package com.arthur.crawler;

import java.io.BufferedReader;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.net.HttpURLConnection;
import java.net.URL;

public class Crawler {

	public Crawler(){
		
	}
	
	public String getHtmlCode(String url){
		String htmlCode = "";
		try{
			
			URL httpUrl = new URL(url);
			HttpURLConnection connection = (HttpURLConnection) httpUrl.openConnection();
//			connection.connect();
			
			InputStream is = connection.getInputStream();
			
			byte[] buffer = new byte[512];
			int length = -1;
			String tmp = "";
			while((length = is.read(buffer, 0, 512)) != -1){
				tmp = new String(buffer, 0, 512);
				htmlCode += tmp;
//				System.out.println(tmp);
			}
			is.close();
			connection.disconnect();
			
		}catch(Exception e){
			e.printStackTrace();
		}
		if(htmlCode == ""){
			System.out.println("ERROR!GET NULL HTML!");
		}
		return htmlCode;
		
	}
	
	
	public String getTextCode(String url){
		String textCode = "";
		try{
			
			URL httpUrl = new URL(url);
			HttpURLConnection connection = (HttpURLConnection) httpUrl.openConnection();
//			connection.connect();
			connection.setRequestProperty("Accept-Charset","GB2312");
			
			InputStream is = connection.getInputStream();
			BufferedReader br = new BufferedReader(new InputStreamReader(is,"GB2312"));
			
			String tmp = "";
			while((tmp = br.readLine()) != null){
				textCode += tmp;
				System.out.println(tmp);
			}
			is.close();
			connection.disconnect();
			
		}catch(Exception e){
			e.printStackTrace();
		}
		if(textCode == ""){
			System.out.println("ERROR!GET NULL HTML!");
		}
		return textCode;
		
	}
	
}


