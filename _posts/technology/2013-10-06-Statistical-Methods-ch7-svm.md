---
layout: default
title: 统计学习方法BR-CH7：支持向量机（SVM）
---
### 一、支持向量机总览

支持向量机（Support Vector Machine，SVM）是一种二类分类模型。

1. 模型

支持向量机的基本模型是：定义在特征空间上的间隔最大的线性分类器。

#### 支持向量机与感知机的不同

感知机是采用误分类最小策略，求得分离超平面，这时的解是无穷多个；支持向量机是间隔最大策略，求得最优分离超平面，这时的解是唯一的。

#### 支持向量机的学习是在特征空间进行的

输入都由输入空间转换到特征空间。其中，线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量；非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。

#### 支持向量机适用的数据集

支持向量机可用于线性可分数据集，利用硬间隔最大化得到线性可分支持向量机；可以用于近似线性可分数据集（即：有线性不可分的特异点，去除后数据集线性可分），利用软间隔最大化得到线性支持向量机（即：包含线性可分和线性不可分情况）；还可以用于线性不可分数据集（即：非线性可分数据集），利用定义一个从原空间到特征空间的非线性转换（映射），将非线性问题变换成线性问题，通过求解变换后的线性问题来解出原来的非线性问题（可以通过核技巧和软间隔最大化进行求解），得到非线性支持向量机。

这个过程是一个构建由简入繁的模型的过程。模型逐渐复杂，但适用的数据集范围不断扩大。后者均可以看做前者的推广，前者可以看做后者的特例。

2. 策略

支持向量机的学习策略就是间隔最大化。

通过数学推导，间隔最大化问题可以形式化为一个求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小化问题。

3. 算法

支持向量机的学习算法是求解凸二次规划的最优化算法。

下面单独介绍三种支持向量机。

二、线性可分支持向量机

1. 定义

给定一个数据集如下：

	T = {(x1,y1),(x2,y2),···,(xn,yn)}
	其中，xi ∈ R<sup>n</sup>, yi ∈ {+1,-1}, i = 1,2,...,N。

我们假定数据集是线性可分的（注意：这是一个很强的条件，与其他支持向量机的区别也在于此）。

给定线性可分训练数据集，通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为：

	w* · x + b* = 0

及相应的分类决策函数：

	f(x) = sign(w* · x + b*)

称为线性可分支持向量机。

其中，根据[List of mathematical symbols](http://en.wikipedia.org/wiki/List_of_mathematical_symbols)，w* 表示w的共轭复数。【存疑：为何要用共轭复数？】

那么，间隔是什么间隔？间隔最大化是怎么最大化？相应的凸二次规划问题又是什么？我们下面分别来讲。

2. 函数间隔和几何间隔

#### 函数间隔（functional margin）

一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面：w·x + b = 0确定的情况下，|w·x+b|能够相对的表示点x距离超平面的远近，而w·x+b的符号与类标记y的符号是否一致能够表示分类是否准确。因此，我们可以用量y(w·x+b)来表示分类的正确性及确信度（结果为正表示分类正确，为负表示分类错误；结果越接近于0表示分类确信度越高），这就是函数间隔的概念。

函数间隔的表示符号为γ^{^}。对于给定的训练数据集T和超平面(w,b)：

定义超平面(w,b)关于样本点(xi,yi)的函数间隔为：

γ_{i}^{^} = y_{i}(w·x_{i} + b)

定义超平面(w,b)关于训练数据集T的函数间隔为，超平面(w,b)关于T中所有样本点的函数间隔的最小值，即：

γ^{^} = min_{i=1,...,N}γ_{i}^{^}

#### 几何间隔（geometric margin）

函数间隔可以表示分类预测的正确性及确信度。但是，选择分离超平面时，只有函数间隔还不够。因为只要成比例的改变w和b（例如：将他们改变成为2w和2b），超平面本身没有发生改变（因为超平面为w·x+b=0，故w和b成比例改变后，超平面本身并不改变），但是函数间隔确实原来的2倍。这启示我们，可以对分离超平面的法向量w加以某种约束，如规范化，||w||=1，使得间隔是确定的。这时的间隔成为几何间隔。

几何间隔的表示符号为γ。对于给定的训练数据集T和超平面(w,b)：

定义超平面(w,b)关于样本点(xi,yi)的几何间隔为：

γ_{i} = y_{i}(\frac{w}{||w||}·x_{i} + \frac{b}{||w||})

定义超平面(w,b)关于训练数据集T的几何间隔为，超平面(w,b)关于T中所有样本点的几何间隔的最小值，即：

γ = min_{i=1,...,N}γ_{i}

#### 函数间隔和几何间隔的关系

我们可以看出，函数间隔γ^{^}与几何间隔γ的关系为：

γ_{i} = \frac{γ^{^}}{||w||}

γ = \frac{γ^{^}}{||w||}

如果||w||=1，那么函数间隔与几何间隔相等。如果超平面的参数w和b等比例的改变（此时超平面没有改变），则函数间隔也等比例改变，而几何间隔不变。因此，一般计算中使用几何间隔而非函数间隔。

3. 间隔最大化

对线性可分数据集而言，线性可分分离超平面有无穷多个（等价于感知机。由于数据集线性可分，若采用误分类最小策略，误分类数目为0，但结果无穷多个）；但是，采用间隔最大化策略，得到几何间隔最大的分离超平面是唯一的（证明见下面）。

间隔最大化的直观解释是：不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将他们分开。这样的哦超平面应该对未知的新实例有很好的分类预测能力。

#### 最大间隔分离超平面

如何求得一个几何间隔最大的分离超平面？这个问题可以表示为下面的约束最优化问题：

max_{w,b}	γ

s.t. 	y_{i}(\frac{w}{||w||}·x_{i} + \frac{b}{||w||}) ≥ γ, 	i=1,2,...,N

理解如下：约束条件表示超平面(w,b)关于每个样本点的几何间隔至少是γ，而我们希望最大化超平面(w,b)关于训练数据集的几何间隔γ（即：求得γ的最大值）。

将此式转换为函数间隔形式，如下：

max_{w,b}	\frac{γ^{^}}{||w||}

s.t. 	y_{i}(w·x_{i} + b) ≥ γ^{^}, 	i=1,2,...,N

上面已经讲过，函数间隔γ^{^}的取值并不会影响最优化问题的解（若w和b改变，则函数间隔改变，超平面和几何间隔不变），这样，可以对上式取γ^{^}=1，代入上式。同时，又由于最大化\frac{1}{||w||}与最小化\frac{1}{2}||w||^{^}相同，转换上式，我们可以得到：

min_{w,b}	\frac{1}{2}||w||^{^}

s.t. 	y_{i}(w·x_{i} + b) - 1 ≥ 0, 	i=1,2,...,N

这是一个凸二次优化问题（convex quadratic programming）。之前的转化也都是为了达成这一步。如果求解出约束最优化问题的w* ,b* ，那么就可以得到最大间隔分离超平面w* · x + b* = 0和分类决策函数f(x) = sign(w* · x + b* )，即可以得到线性可分支持向量机模型。

#### 最大间隔分离超平面的存在唯一性

《统计学习方法》书中P100页证明了线性可分数据集的最大间隔分离超平面的存在性和唯一性。我们在此证明略过，需要时可以查阅书。

#### 支持向量和间隔边界

在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。支持向量是使约束条件式的等号成立的点，即：

y_{i}(w·x_{i} + b) - 1 = 0

分离超平面在正负支持向量实例中间。分别经过正负支持向量，并且平行于分离超平面的平面称为间隔边界。间隔边界之间的距离称为间隔（margin）。

间隔依赖于分离超平面的法向量w。我们知道，在支持向量时，几何间隔达到最小。由上可知，此时函数间隔r^{^}定义为1，由此可得：

γ^{^} = 1

∴ γ · ||w|| = 1 即： γ = \frac{1}{||w||}

此时，几何间隔(margin)为\frac{1}{||w||}，间隔为2γ = \frac{2}{||w||}。

在决定分离超平面时只有支持向量起作用，其他实例点并不起作用。正因为如此，所以将这种分类模型成为支持向量机。支持向量的个数一般很少，因此，支持向量机是由很少的“重要的”训练样本决定。

4. 学习的对偶算法

对原始的优化问题，通过拉格朗日对偶性，求解对偶问题从而得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。

这样做的优点：1. 对偶问题往往更容易求解；2. 自然引入核函数，进而推广到非线性分类问题。

此处不讲了，因为我对偶算法我还没看懂。有需要的请参考《统计学习方法》P103页。


### 三、线性支持向量机




### 四、非线性支持向量机

### 五、总结



参考资料：

