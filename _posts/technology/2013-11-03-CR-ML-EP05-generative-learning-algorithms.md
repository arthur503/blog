---
layout: default
title: 机器学习CR-EP05：生成学习算法
---
这一节主讲的内容包括：

1. Generative Learnging Algorithms
2. Gaussion Discriminant Analysis
3. Digression Gaussions
4. Generative VS Discrimitive comparision
5. Naive Bayes
6. Laplace Smoothing

之间将的判别学习方法（如线性回归、逻辑回归等），是画出一条直线，将两类点分开。今天要讲的是生成学习算法，与之前的不同。

* 判别学习方法：是在x已知的情况下，学习p(y|x)或h_{θ}(x)函数(∈\{0,1\})。
* 生成学习方法：是在已知class label y的情况下，对p(x|y)进行建模（计算时可能还需要p(y)）。

下面讲第一个生成学习算法的例子。

##### Gaussion Discriminant Analysis 高斯判别分析模型

先回顾了下多元高斯分布的表达式，之后假设class label y之间为伯努利分布得到P(y)、P(x|y=0)、P(x|y=1)的表达式，计算出joint likelyhood：l(φ、μ0、μ1、ε)，对其进行极大似然估计，得到模型。之后，可以利用模型进行预测，假设P(y)为均匀分布，可得最后预测表达式为：arg max_{y} P(x|y)。

##### Gaussion Discriminant Analysis与Logistic Regression之间的关系

以良性/恶性肿瘤判别为例，GDA模型中，需要对正(y=1)、负样本(y=0)均建立模型，之后对给定样本x进行判断概率：P(y=1|x)，即：对于给定的x，判断其为良性肿瘤(y=1)的概率是多少。

根据Andrew画出的模型图可以看出，最后P(y)的曲线是近似与sigmod函数曲线的，也就是与逻辑回归判别函数曲线相似（但并不相同！二者的陡峭程度等不同），这是很有意思的一件事。

##### 生成学习方法的优缺点

在GDA的算法中，x|y符合高斯分布是一个很强的条件，可以据此推出逻辑回归的表达式。但是，反方向不成立。因此，如果已知数据集符合高斯分布的话，使用GDA算法效果更好，因为它可以利用“高斯分布”这个强烈的条件，得到更好的拟合模型；但是，若不确定数据集是否符合高斯分布的话，使用逻辑回归算法效果更好，因为逻辑回归算法具有更强的健壮性，适用于更广泛的情况。

生成学习算法的真正好处在于其需要更少的数据。虽然大多数情况下，数据并非严格符合高斯分布，但是一般是近似符合的，这样，可以利用更少的数据来拟合出一个还不错的模型；而逻辑回归算法由于其假设更弱一些，所以健壮性更强，不过为了拟合出模型，其需要更多的样本。

下面来讲第二个生成学习算法的例子。

##### Naive Bayes Algorithm 朴素贝叶斯算法

这里用到了垃圾邮件判别的例子。

首先来看，如何用向量来表示文本？

由于文本是由单词构成的，因此，我们可以用文本中是否出现某个单词来表示。假设我们有个单词的词典（如：近2个月来所有邮件中出现3次以上的单词构成的词典），长度为50000个，那么，我们可以用一个50000维的向量来表示一段文本。文本中出现了某个单词，则将向量中的对应位置一，否则置零。

我们假设文本中各个单词之间的出现是独立的（现实中肯定是相关的，但Andrew说从实际效果上来看结果还是不错的），可以计算得到：垃圾邮件的比例φ_{y}，垃圾邮件中向量的第j个单词出现的概率φ_{j
|y=1}等，据此，可以计算出：在已知文本中单词出现情况下，文本为垃圾邮件的概率：P(y|x)。

问题到这里算是一个结束了，但是还有个小小的意外。就是：如果出现了某个新词，假如说是“NIPS”，且其在向量中为第30000个单词，那么可得：P(x_{30000}|y=0) = 0, P(x_{30000}|y=1) = 0，则最后计算得：P(y=1|x) = 0/(0+0)，无解。

应该如何改进呢？这里就要引入Laplace Smoothing。

##### Laplace Smoothing

我们举个例子，假设两只足球队A和B，已知两队之前的比赛中，A输5场，赢0场，那么，推测下一场比赛，A队获胜的概率是多少？

按照之前的算法，P(y=1) = frac{y=1的个数}{y=0的个数 + y=1的个数} = 0/(5+0) = 0

我们对此作出改进，使用拉普拉斯平滑，得到：P(y=1) = frac{y=1的个数+1}{y=0的个数+1 + y=1的个数+1} = (0+1)/(5+1 + 0+1) = 1/7

这样的结果看起来就舒服多了。虽然A队在之前输了5场，赢了0场，但也不能说之后就没有机会赢对吧？1/7的胜率比较符合常理。

其中，Laplace Smoothing更一般的表示为：如果y有k中可能，那么在结果中，分子+1，分母+k。

好了，我们回到上一个垃圾邮件的例子中来，利用Laplace Smoothing对P(y=1|x)进行顺滑，得到的结果为：φ_{300000
|y=1} = (0+1)/(0+2) = 1/2。也就是说，对于未出现过的单词，根据此判断其为垃圾邮件的概率为1/2。